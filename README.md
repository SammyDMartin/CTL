## Location of Results

The Experiments in the thesis relating to comparing the ABCD(2013) and CTL02 are found in the Folder CTL02 Baseline Tests - plots of the Final fitted models for ABCD(2013) and CTL02. The original training data is located in training_data_real. The textree files contain the raw output of CTL00 running the CTL02 and ABCD(2013) models, and the graphs have the BIC scores in their filenames.

The CTL04 Full Sweeps contain the runs of the metropolis-hastings used for structure recovery - Linear and RBF, RBF Periodic runs.

The 'Fit for' folders In each subfolder contain the CTL02 Runs that provided the initial source of counts for the Linear and RBF, RBF + Periodic cases. the original training data is located in training_data_linear (for Linear) and training_data_comp (for Compositional).

Within each subfolder in CTL04 Full Sweeps is a Grammar folder containing the Grammar, Summary folder containing the fitted models and a Plot image showing the result of the MH sequence, along with a report.md file showing convergence behaviour.

The results.md files in each Summary subfolder were used to produce the tables seen in the Thesis, and were produced by CTL-modal.py. The U_comp plot images were produced by CTL-reader.py

If you wish to rerun the MH searches, then move the desired Grammar folder to the top level directory and specify its equivalent folder for training data.

CTL05 Predictions contains plots and textree output logs for searches run by extracting the posterior predictive models for CTL04

# CTL
This repo contains material related to my MSc research project, *Compositional Transfer Learning in Time Series*. Data taken from https://github.com/jamesrobertlloyd/gp-structure-search/tree/master/data
# CTL01.py
An attempt to implement Automatic Bayesian Covariance Discovery (ABCD) using gpytorch and pytorch.
Methods for automatically producing a compositional kernel to describe 1D data have been proposed, for example https://github.com/jamesrobertlloyd/gpss-research. This repo contains a re-implementation of that software using fast modern libraries for fitting the kernels, and an open ended greedy search over kernels to predict time series data. The predictions are stable up to N=2000 without the approximation or grid interpolation applied. It's probably a good idea to switch those on for larger data sets.
Run the file CTL01.py to perform a compositional kernel search on the synthetic data generated by KernelGen.py. Change the variable TEST in KernelGen to any other function to test on synthetic data.
Replace the variable train_y in the __main__ function of CTL01.py with a torch tensor containing your 1D data to perform the search on any real data.
The output file with the lowest loss function in the folder is the final fitted model.
# CTLGPy.py
A more stable implementation of ABCD that uses the slower GPy instead of GPYtorch. Works the same way as CTL01 except that you must use CTL02 to load data into it. A wrapper around GPy that defines the class CompositionalGPyModel(), which will fit a compositional kernel to a dictionary of Inputs produced by CTL02 or CTL04
# CTL02.py
This is the ABCD Implementation - after a given run it will save to a folder all fitted model trees (as tree .pickle file), plots of all Kernels found to be optimal in the greedy search and txt files containing an output log of the greedy search results.
Run with args:
-mode cross_validation_loss, validation_loss, loss, BIC will perform search with one of those options as utility scores. The number of k-folds can be found in __main__
-lim the depth limit of the search - set to between 4 and 10 for good results
-th the threshold (fraction of change of score) at which to terminate search. Set to 1e-2 when using raw loss values to avoid overfitting
-F The name of the folder containing training data 
-plot 1 to save plots, 0 otherwise.
# CTL03.py
Ingests all tree pickle files in folder 'Trees' and builds the transition probabilities for the kernel grammar, saving it to folder 'Summary' along with descriptive report 'out.md'. Note that to use CTL04, you must use CTL03C first, NOT CTL03 (because the grammars it produces are not leave-one-out). CTL03 is just used to provide a quick summary and a human-readable report of the PCFG prior for a given set of Tree parses.
# CTL03C.py
Run this when you have decided on the time series to be used in testing. Save all Trees produced by CTL02 that you want to use as counts sources to folder Trees, then run CTL03C. It will produce grammar files to be used by CTL04 and save them to the folder Grammars. The name of each file will correspond to the timeseries it is intended for.
# CTL04.py
This is the CTL Algorithm implementation - It performs metropolis-Hastings sweeps in parallel over timeseries given timeseries specific PCFG priors using multiprocessing, saving the results to folder Summary as pickle files which must be read to be interpreted (by CTL-modal and CTL-reader). Requires A grammar file matching each time series .mat file to run.
Run with args:
-g Location of Grammar folder
-F The name of the folder containing training data
Will save the fitted model pickle files to Folder Summary. Use CTL-reader and CTL-modal to scan them and interpret results.

# CTL-reader
Change the value of TRUE and MH_seqs global variables in CTL-reader to the Kernel you want to assess the sequences for and the folder containing the sequences made by CTL04 (usually Summary, unless you have moved them). It will plot the frequency of the Kernel in the Informed and Uninformed cases. Was used to produce final plots for Thesis.

# CTL-modal
Change the value of Greedy_seqs, Grammar_files, MH_seqs in CTL-modal to the location of the initial ABCD fits, the Grammar rule files and the MH Sequences for the same time series. First should be produced by CTL02 (tree pickle files), second by CTL03C (grammar files) and third by CTL04 (in Summary). Will produce and save a report comparing the frequency of the Ground truth structure (specified in global variable TRUE) and MAP estimates of each model. Was used to produce the tables describing structure recovery.

# CTL05, CTL00
These are both used to produce single fitted models. CTL00 fits a series of specific models specified in its __main__ method taken from previous CTL02 runs for use in comparing ABCD implementations. CTL05 will read a MH sequence and fit the modal model to a test data file.

# KernelGen
Used to unpack the .mat files used by CTL02 and CTL04.